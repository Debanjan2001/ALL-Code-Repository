{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "- Contributed by : Debanjan Saha\n",
        "- Running on : Local Machine\n"
      ],
      "metadata": {
        "id": "U-sRzyQsUvBv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Neural Network based Language Model"
      ],
      "metadata": {
        "id": "G-6p-yxhVwnh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "from torch.nn.utils import clip_grad_norm_\n",
        "import math"
      ],
      "metadata": {
        "id": "4STKCfy4bxYR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# directory for saving model\n",
        "!mkdir -p \"saved_model\""
      ],
      "metadata": {
        "id": "wCYwXqn4Kjx2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def load_dataset():\n",
        "    \"\"\"\n",
        "    Load dataset from the folder 'data'\n",
        "    \"\"\"\n",
        "    with open(\"./data/train.txt\", 'r') as f:\n",
        "        train_data = [line.strip() for line in f.readlines()]\n",
        "\n",
        "    with open(\"./data/test.txt\", 'r') as f:\n",
        "        test_data = [line.strip() for line in f.readlines()]\n",
        "\n",
        "    return train_data, test_data"
      ],
      "metadata": {
        "id": "zdN4HaL7WNCT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_sents, test_sents = load_dataset()"
      ],
      "metadata": {
        "id": "pvEVVlimWPGe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_sents[:3]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b2HfcF66djy8",
        "outputId": "f3175101-7499-4329-ef64-6f07b0456506"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['liberty all star usa sets initial payout',\n",
              " 'we are being accused of not implementing this agreement',\n",
              " 'entregrowth closed at 135 dlrs and options at 55 cents']"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_sents[:3]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8ejkFZwOdlhh",
        "outputId": "5793d582-484c-4ae6-8a40-eebb7e6dfa3e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['the company said each debenture is convertible into shares of businessland common stock at a conversion price of 2050 dlrs',\n",
              " 'sumita says he does not expect further dollar fall',\n",
              " 'the tin price is likely to rise to 20 ringgit a kilo this year because of the producers accord on export quotas and the reluctance of brokers and banks to sell the metal at lower prices a malaysian government bulletin said']"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Hyper-parameters (Obtained after tuning them)\n",
        "hyperparams = {\n",
        "    \"embed_size\" : 128,\n",
        "    \"hidden_size\" : 1024,\n",
        "    \"num_layers\" : 1,\n",
        "    \"num_epochs\" : 5,\n",
        "    \"batch_size\" : 20,\n",
        "    \"seq_length\" : 30,\n",
        "    \"learning_rate\" : 0.002,\n",
        "}"
      ],
      "metadata": {
        "id": "pYH1iSafjE3d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tokenize import sent_tokenize, word_tokenize\n",
        "import gensim\n",
        "from gensim.models import Word2Vec"
      ],
      "metadata": {
        "id": "aQganrsB7ZZE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class CustomDataset:\n",
        "    \"\"\"\n",
        "    Custom Dataset class to \n",
        "    - split train data into 90:10 train-dev split\n",
        "    - batchify train, test, dev data\n",
        "    - generate corpus using a word2vec model \n",
        "    \"\"\"\n",
        "    def __init__(self, train_sents, test_sents, batch_size=hyperparams[\"batch_size\"]):\n",
        "        self.train_sents = train_sents\n",
        "        self.test_sents = test_sents\n",
        "        self.batch_size = batch_size\n",
        "        self.generate_corpus()\n",
        "        \n",
        "    def generate_corpus(self):\n",
        "        \"\"\"\n",
        "        Generates the corpus by tokenizing the words in each sentence,\n",
        "        Makes a word2vec model on the tokens and generates a pretrained word2vec embedding for the corpus\n",
        "        \"\"\"\n",
        "\n",
        "        data = []\n",
        "        # iterate through each sentence in the training sentences\n",
        "        for sentence in self.train_sents:\n",
        "            cur_sent = []\n",
        "            # tokenize the sentence into words\n",
        "            for word in sentence.split():\n",
        "                cur_sent.append(word.lower())\n",
        "            data.append(cur_sent)\n",
        "\n",
        "        self.w2v_model = gensim.models.Word2Vec(data, min_count=1, vector_size=hyperparams[\"embed_size\"], window=5, workers=4)\n",
        "        self.word2idx = self.w2v_model.wv.key_to_index\n",
        "        self.idx2word = self.w2v_model.wv.index_to_key\n",
        "        self.vocab_size = len(self.word2idx)\n",
        "        \n",
        "        num_tokens = sum([len(words_array) for words_array in data])\n",
        "        self.ids = torch.LongTensor(num_tokens)\n",
        "        \n",
        "        index = 0\n",
        "        for sentence in data:\n",
        "            for word in sentence:\n",
        "                self.ids[index] = self.word2idx[word]\n",
        "                index += 1\n",
        "        \n",
        "    def create_batch(self, array, batch_size):\n",
        "        \"\"\"\n",
        "        creates batches of input array in given batch_size \n",
        "        \"\"\"\n",
        "        batched_total_size = (array.size(0) // batch_size) * batch_size \n",
        "        array = array[:batched_total_size]\n",
        "        return array.view(batch_size, -1)\n",
        "\n",
        "    def get_batched_train_and_dev_data(self, train_split=0.9):\n",
        "        \"\"\"\n",
        "        splits the train data into 90:10 ratio for train-dev set\n",
        "        batchifies both train and dev data for future use\n",
        "        \"\"\"\n",
        "        train_len = int(train_split * self.ids.size(0))\n",
        "        train_data, dev_data = self.ids[:train_len], self.ids[train_len:]\n",
        "        train_data, dev_data = self.create_batch(train_data, self.batch_size), self.create_batch(dev_data, self.batch_size)\n",
        "        return train_data, dev_data \n",
        "\n",
        "    def get_batched_test_data(self):\n",
        "        \"\"\"\n",
        "        generates test data tokens and batchifies the data for testing the model\n",
        "        \"\"\"\n",
        "        test_words = []\n",
        "        for sent in self.test_sents:\n",
        "            for word in sent.split():\n",
        "                if word in self.word2idx:\n",
        "                    test_words.append(word.lower())\n",
        "\n",
        "        test_data = torch.LongTensor(len(test_words))\n",
        "        index = 0\n",
        "        for word in test_words:\n",
        "            test_data[index] = self.word2idx[word]\n",
        "            index += 1\n",
        "\n",
        "        test_data = self.create_batch(test_data, self.batch_size)\n",
        "        return test_data\n",
        "\n",
        "    def get_vocab_size(self):\n",
        "        \"\"\"\n",
        "        returns vocab size for the dataset\n",
        "        \"\"\"\n",
        "        return self.vocab_size"
      ],
      "metadata": {
        "id": "pyOvIyvUik8T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = CustomDataset(train_sents, test_sents, batch_size=hyperparams[\"batch_size\"])"
      ],
      "metadata": {
        "id": "y3kzQ6K9Blwe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_data, dev_data = dataset.get_batched_train_and_dev_data(train_split=0.9)"
      ],
      "metadata": {
        "id": "1YZVvsDICr9p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_data.shape, dev_data.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b9a1aoJLENSI",
        "outputId": "a6084c47-ea4c-4349-dfaa-5927fe8a0560"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(torch.Size([20, 60649]), torch.Size([20, 6738]))"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_data = dataset.get_batched_test_data()"
      ],
      "metadata": {
        "id": "ndD6mPsxE1wc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_data.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YzGAtTslFP3X",
        "outputId": "f6977b6f-9af3-4b9c-bb3d-1a21c3d47e32"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([20, 16507])"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dataset.get_vocab_size()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4KVGXbXCFxHY",
        "outputId": "c27721f9-2a91-4202-d014-12e779958a3e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "44689"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Device configuration\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "device"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "44d388ed-6603-429f-9858-87aedacfdb7b",
        "id": "E8lB6IiQV8Cx"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "device(type='cuda')"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(dataset.get_vocab_size())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cf340d13-cf9e-4542-87c3-e3ca6dd95f2e",
        "id": "Gp3K-qcqV8C2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "44689\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pretrained_w2v_embeddings = torch.FloatTensor(dataset.w2v_model.wv.vectors)"
      ],
      "metadata": {
        "id": "tftcXw1gJAUu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# RNN based language model\n",
        "class RNNLanguageModel(nn.Module):\n",
        "    def __init__(self, embed_size, hidden_size, num_layers, vocab_size):\n",
        "        super(RNNLanguageModel, self).__init__()\n",
        "        self.embed = nn.Embedding.from_pretrained(pretrained_w2v_embeddings)\n",
        "        self.lstm = nn.LSTM(embed_size, hidden_size, num_layers, batch_first=True)\n",
        "        self.linear = nn.Linear(hidden_size, vocab_size)\n",
        "        \n",
        "    def forward(self, x, h):\n",
        "        # Embed word ids to vectors\n",
        "        x = self.embed(x)\n",
        "        \n",
        "        # Forward propagate LSTM\n",
        "        out, (h, c) = self.lstm(x, h)\n",
        "        \n",
        "        # Reshape output to (batch_size*sequence_length, hidden_size)\n",
        "        out = out.reshape(out.size(0)*out.size(1), out.size(2))\n",
        "        \n",
        "        # Decode hidden states of all time steps\n",
        "        out = self.linear(out)\n",
        "        return out, (h, c)"
      ],
      "metadata": {
        "id": "lw3dB8ifV8DB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def detach(states):\n",
        "    return [state.detach() for state in states]"
      ],
      "metadata": {
        "id": "Tjscc3EhLM38"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = RNNLanguageModel(\n",
        "    embed_size=hyperparams[\"embed_size\"], \n",
        "    hidden_size=hyperparams[\"hidden_size\"], \n",
        "    num_layers=hyperparams[\"num_layers\"],\n",
        "    vocab_size=dataset.get_vocab_size(),\n",
        ").to(device)\n",
        "\n",
        "# Loss and optimizer\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=hyperparams[\"learning_rate\"])"
      ],
      "metadata": {
        "id": "7vn97dUhOscy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "seq_length = hyperparams[\"seq_length\"]\n",
        "batch_size = hyperparams[\"batch_size\"]\n",
        "num_layers = hyperparams[\"num_layers\"]\n",
        "hidden_size = hyperparams[\"hidden_size\"]\n",
        "num_epochs = hyperparams[\"num_epochs\"]"
      ],
      "metadata": {
        "id": "DoOT72WEOzt8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "best_loss = math.inf\n",
        "\n",
        "num_train_batches = train_data.size(1) // seq_length\n",
        "num_dev_batches = dev_data.size(1) // seq_length\n",
        "\n",
        "# Train the model\n",
        "for epoch in range(num_epochs):\n",
        "    # Set initial hidden and cell states\n",
        "    states = (torch.zeros(num_layers, batch_size, hidden_size).to(device),\n",
        "              torch.zeros(num_layers, batch_size, hidden_size).to(device))\n",
        "    \n",
        "\n",
        "    print(\"Currently Running: Training Set\")\n",
        "    model.train()\n",
        "    for i in range(0, train_data.size(1) - seq_length, seq_length):\n",
        "        # Get mini-batch inputs and targets\n",
        "        inputs = train_data[:, i:i+seq_length].to(device)\n",
        "        targets = train_data[:, (i+1):(i+1)+seq_length].to(device)\n",
        "        \n",
        "        # Forward pass\n",
        "        states = detach(states)\n",
        "        outputs, states = model(inputs, states)\n",
        "        loss = criterion(outputs, targets.reshape(-1))\n",
        "        \n",
        "        # Backward and optimize\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        clip_grad_norm_(model.parameters(), 0.5)\n",
        "        optimizer.step()\n",
        "\n",
        "        step = (i+1) // seq_length\n",
        "        if step % 100 == 0:\n",
        "            print ('Epoch [{}/{}], Step[{}/{}], Loss: {:.4f}, Perplexity: {:5.2f}'\n",
        "                   .format(epoch+1, num_epochs, step, num_train_batches, loss.item(), np.exp(loss.item())))\n",
        "\n",
        "    total_loss = 0.0\n",
        "\n",
        "    states = (torch.zeros(num_layers, batch_size, hidden_size).to(device),\n",
        "              torch.zeros(num_layers, batch_size, hidden_size).to(device))\n",
        "    \n",
        "    print(\"Currently Running: Development Set\")\n",
        "    model.eval()\n",
        "    for i in range(0, dev_data.size(1) - seq_length, seq_length):\n",
        "        # Get mini-batch inputs and targets\n",
        "        inputs = dev_data[:, i:i+seq_length].to(device)\n",
        "        targets = dev_data[:, (i+1):(i+1)+seq_length].to(device)\n",
        "\n",
        "        # Forward pass\n",
        "        states = detach(states)\n",
        "        outputs, states = model(inputs, states)\n",
        "        loss = criterion(outputs, targets.reshape(-1))\n",
        "\n",
        "        total_loss += loss.item()\n",
        "\n",
        "        step = (i+1) // seq_length\n",
        "        if step % 100 == 0:\n",
        "            print ('Epoch [{}/{}], Step[{}/{}], Loss: {:.4f}, Perplexity: {:5.2f}'\n",
        "                    .format(epoch+1, num_epochs, step, num_dev_batches, loss.item(), np.exp(loss.item())))\n",
        "\n",
        "    # print(f\"Total Loss: {total_loss}, best_loss: {best_loss}\")\n",
        "\n",
        "    if total_loss < best_loss:\n",
        "        best_loss = total_loss\n",
        "        print(\"Better Loss! Saving Model...\")\n",
        "        # Save the model checkpoints\n",
        "        torch.save(model.state_dict(), 'saved_model/rnn_model.ckpt')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "Mahq4bODK_xt",
        "outputId": "72585c5a-1a95-4a0f-fbec-89a75116f3e3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Currently Running: Training Set\n",
            "Epoch [1/5], Step[0/2021], Loss: 10.7083, Perplexity: 44723.78\n",
            "Epoch [1/5], Step[100/2021], Loss: 6.9827, Perplexity: 1077.81\n",
            "Epoch [1/5], Step[200/2021], Loss: 5.9926, Perplexity: 400.44\n",
            "Epoch [1/5], Step[300/2021], Loss: 6.0417, Perplexity: 420.62\n",
            "Epoch [1/5], Step[400/2021], Loss: 5.8207, Perplexity: 337.20\n",
            "Epoch [1/5], Step[500/2021], Loss: 5.7953, Perplexity: 328.74\n",
            "Epoch [1/5], Step[600/2021], Loss: 5.4784, Perplexity: 239.47\n",
            "Epoch [1/5], Step[700/2021], Loss: 5.4416, Perplexity: 230.82\n",
            "Epoch [1/5], Step[800/2021], Loss: 5.3196, Perplexity: 204.29\n",
            "Epoch [1/5], Step[900/2021], Loss: 5.2324, Perplexity: 187.24\n",
            "Epoch [1/5], Step[1000/2021], Loss: 5.1583, Perplexity: 173.87\n",
            "Epoch [1/5], Step[1100/2021], Loss: 5.4210, Perplexity: 226.11\n",
            "Epoch [1/5], Step[1200/2021], Loss: 5.2585, Perplexity: 192.18\n",
            "Epoch [1/5], Step[1300/2021], Loss: 4.9296, Perplexity: 138.33\n",
            "Epoch [1/5], Step[1400/2021], Loss: 5.4087, Perplexity: 223.34\n",
            "Epoch [1/5], Step[1500/2021], Loss: 4.9774, Perplexity: 145.09\n",
            "Epoch [1/5], Step[1600/2021], Loss: 5.4852, Perplexity: 241.10\n",
            "Epoch [1/5], Step[1700/2021], Loss: 5.3002, Perplexity: 200.38\n",
            "Epoch [1/5], Step[1800/2021], Loss: 4.9277, Perplexity: 138.06\n",
            "Epoch [1/5], Step[1900/2021], Loss: 4.9418, Perplexity: 140.02\n",
            "Epoch [1/5], Step[2000/2021], Loss: 5.2461, Perplexity: 189.82\n",
            "Currently Running: Development Set\n",
            "Epoch [1/5], Step[0/224], Loss: 4.8346, Perplexity: 125.78\n",
            "Epoch [1/5], Step[100/224], Loss: 4.9764, Perplexity: 144.95\n",
            "Epoch [1/5], Step[200/224], Loss: 5.2220, Perplexity: 185.30\n",
            "Better Loss! Saving Model...\n",
            "Currently Running: Training Set\n",
            "Epoch [2/5], Step[0/2021], Loss: 5.3100, Perplexity: 202.35\n",
            "Epoch [2/5], Step[100/2021], Loss: 5.2900, Perplexity: 198.34\n",
            "Epoch [2/5], Step[200/2021], Loss: 4.7351, Perplexity: 113.87\n",
            "Epoch [2/5], Step[300/2021], Loss: 5.0487, Perplexity: 155.81\n",
            "Epoch [2/5], Step[400/2021], Loss: 4.8048, Perplexity: 122.09\n",
            "Epoch [2/5], Step[500/2021], Loss: 4.4984, Perplexity: 89.87\n",
            "Epoch [2/5], Step[600/2021], Loss: 4.4847, Perplexity: 88.65\n",
            "Epoch [2/5], Step[700/2021], Loss: 4.5308, Perplexity: 92.84\n",
            "Epoch [2/5], Step[800/2021], Loss: 4.4803, Perplexity: 88.26\n",
            "Epoch [2/5], Step[900/2021], Loss: 4.3689, Perplexity: 78.96\n",
            "Epoch [2/5], Step[1000/2021], Loss: 4.2569, Perplexity: 70.59\n",
            "Epoch [2/5], Step[1100/2021], Loss: 4.4720, Perplexity: 87.53\n",
            "Epoch [2/5], Step[1200/2021], Loss: 4.4301, Perplexity: 83.94\n",
            "Epoch [2/5], Step[1300/2021], Loss: 4.0732, Perplexity: 58.75\n",
            "Epoch [2/5], Step[1400/2021], Loss: 4.5783, Perplexity: 97.35\n",
            "Epoch [2/5], Step[1500/2021], Loss: 4.2340, Perplexity: 69.00\n",
            "Epoch [2/5], Step[1600/2021], Loss: 4.7247, Perplexity: 112.70\n",
            "Epoch [2/5], Step[1700/2021], Loss: 4.4967, Perplexity: 89.72\n",
            "Epoch [2/5], Step[1800/2021], Loss: 4.0735, Perplexity: 58.76\n",
            "Epoch [2/5], Step[1900/2021], Loss: 4.2618, Perplexity: 70.94\n",
            "Epoch [2/5], Step[2000/2021], Loss: 4.5495, Perplexity: 94.58\n",
            "Currently Running: Development Set\n",
            "Epoch [2/5], Step[0/224], Loss: 4.8035, Perplexity: 121.93\n",
            "Epoch [2/5], Step[100/224], Loss: 4.8518, Perplexity: 127.98\n",
            "Epoch [2/5], Step[200/224], Loss: 5.2373, Perplexity: 188.16\n",
            "Currently Running: Training Set\n",
            "Epoch [3/5], Step[0/2021], Loss: 4.6279, Perplexity: 102.30\n",
            "Epoch [3/5], Step[100/2021], Loss: 4.4672, Perplexity: 87.12\n",
            "Epoch [3/5], Step[200/2021], Loss: 4.1791, Perplexity: 65.31\n",
            "Epoch [3/5], Step[300/2021], Loss: 4.4166, Perplexity: 82.81\n",
            "Epoch [3/5], Step[400/2021], Loss: 4.2542, Perplexity: 70.40\n",
            "Epoch [3/5], Step[500/2021], Loss: 3.7803, Perplexity: 43.83\n",
            "Epoch [3/5], Step[600/2021], Loss: 3.8697, Perplexity: 47.93\n",
            "Epoch [3/5], Step[700/2021], Loss: 3.8720, Perplexity: 48.04\n",
            "Epoch [3/5], Step[800/2021], Loss: 3.9172, Perplexity: 50.26\n",
            "Epoch [3/5], Step[900/2021], Loss: 3.7791, Perplexity: 43.78\n",
            "Epoch [3/5], Step[1000/2021], Loss: 3.5962, Perplexity: 36.46\n",
            "Epoch [3/5], Step[1100/2021], Loss: 3.8124, Perplexity: 45.26\n",
            "Epoch [3/5], Step[1200/2021], Loss: 3.8098, Perplexity: 45.14\n",
            "Epoch [3/5], Step[1300/2021], Loss: 3.4827, Perplexity: 32.55\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipykernel_65579/1809017011.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mseq_length\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseq_length\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m         \u001b[0;31m# Get mini-batch inputs and targets\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m         \u001b[0minputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mseq_length\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m         \u001b[0mtargets\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mseq_length\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# del model\n",
        "# torch.cuda.empty_cache()"
      ],
      "metadata": {
        "id": "b7p8hU1aV8DE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.load_state_dict(torch.load(\"saved_model/rnn_model.ckpt\"))\n",
        "model.to(device)"
      ],
      "metadata": {
        "id": "RwQF4knAbSL9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Test the model\n",
        "with torch.no_grad():\n",
        "    total_perplexity = 0.0\n",
        "    sequence_counter = 0\n",
        "\n",
        "    states = (torch.zeros(num_layers, batch_size, hidden_size).to(device),\n",
        "            torch.zeros(num_layers, batch_size, hidden_size).to(device))\n",
        "\n",
        "    num_test_batches = test_data.size(1) // seq_length\n",
        "\n",
        "    model.eval()\n",
        "    for i in range(0, test_data.size(1) - seq_length, seq_length):\n",
        "        # Get mini-batch inputs and targets\n",
        "        inputs = test_data[:, i:i+seq_length].to(device)\n",
        "        targets = test_data[:, (i+1):(i+1)+seq_length].to(device)\n",
        "\n",
        "        # Forward pass\n",
        "        states = detach(states)\n",
        "        outputs, states = model(inputs, states)\n",
        "        loss = criterion(outputs, targets.reshape(-1))\n",
        "\n",
        "        current_seq_perplexity = np.exp(loss.item())\n",
        "        total_perplexity += current_seq_perplexity\n",
        "        sequence_counter += 1\n",
        "\n",
        "        step = (i+1) // seq_length\n",
        "        if step % 100 == 0:\n",
        "            print ('Step[{}/{}], Perplexity: {:5.2f}'.format(step, num_test_batches, current_seq_perplexity))\n",
        "    \n",
        "    print(f\"Test Set Perplexity: {total_perplexity / sequence_counter}\")"
      ],
      "metadata": {
        "id": "0-vkUN64V8DE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "BJRGjBLAT4E3"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}